---
title: "DATA607 Project 3 – Data Science Skills"
author: "Joe Connolly, Gabriel Campos, Gabriella Martinez and Peter Gatica"
date: "`r format(Sys.Date(), '%B %d %Y')`"
output:
  html_document:
    includes:
      in_header: header.html
    css: ./lab.css
    highlight: pygments
    theme: darkly
    toc: true
    toc_float: true
  prettydoc::html_pretty:
    theme: cayman
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

<!-- This is Joe's file --->

<!--match.arg(theme, themes()) : 
    'arg' should be one of "default", "cerulean", "journal","flatly",
    "darkly", "readable", "spacelab", "united", "cosmo", "lumen", "paper", 
    "sandstone", "simplex", "yeti"  -------->

```{r, echo=FALSE,warning=FALSE, results='hide', include=FALSE}
library(devtools)
library(tidyverse)
library(RCurl)
library(plyr)
library(knitr)
library(RMySQL)
library(DSI)
library(odbc)
library(data.table)
library(tau)
library(muRL)
```

<!-- (https://rdrr.io/cran/reactable/man/reactable.html) -->

### Project 3 {.tabset .tabset-pills}



#### Assignment Requirements{.tabset}


This is a project for your entire class section to work on together, since being able to work effectively on a virtual team is a key *“soft skill”* for data scientists. 

Please **note especially** the **requirement** about making a **presentation** during our first meetup after the project is due.

        W. Edwards Deming said, “In God we trust, all others must bring data.” 

**Please use data to answer the question:**

        “Which are the most valued data science skills?”

Consider your work as an exploration:
there is **not** necessarily a *“right answer.”*

#### Grading rubric

* You will need to determine what **tool(s)** you’ll use as a group to effectively collaborate, **share code** and any **project documentation** *(such as motivation, approach, findings).*
* You will have to determine **what data** to collect, **where the data** can be found, and **how to load** it.
* The data that you decide to collect should reside in a **relational database**, in a set of **normalized tables**.
* You should perform any needed **tidying**, **transformations**, and **exploratory data analysis** in `R`.
* Your deliverable should include all **code**, **results**, and **documentation** of your motivation, approach, and findings.
* As a group, you should appoint $(at\ least)$ *three people* to **lead parts** of the presentation.
* While you are strongly encouraged (and will hopefully find it fun) to try out statistics and data models, your **grade will NOT be affected by the statistical analysis and modeling performed** (since this is a semester one course on Data Acquisition and Management).
* **Every** student must be prepared to **explain how the data** was **collected**, **loaded**, **transformed**, **tidied**, and **analyzed for outliers**, etc. in our *Meetup*.
    + This is the **only way I’ll have to determine that everyone actively participated in the process, so you need to hold yourself responsible for understanding what your class-size team did!*
* If you are **unable to attend the meet up**, then you need to either **present to me one-on-one before the meetup presentation**, or **post a 3 to 5 minute video (e.g. on YouTube) explaining the process**.
    + Individual students will not be responsible for explaining any forays into statistical analysis, modeling, data mining, regression, decision trees, etc.
    + You are encouraged to start early, ask many questions, actively post on the provided discussion forum, etc.


#### Notes

1. 
2. 
3. 
4. 





### Database Import {.tabset .tabset-pills}

#### Connect to local Database 

Database `job` is created on each team members local machine using MySQL. Using an **Open Data Base Connectivity (ODBC)** Driver. A connection is established using the **Data Source Name (DSN)** linking `R` to the appropriate user with `SELECT` and `INSERT` privileges. 

```{r MySQL Connect Function}
  db_conn <- dbConnect(odbc(),"data607")
    dbListTables(db_conn)
```

```{r List the fields in the location dimension database table}
# table location_dim fields are listed below
dbListFields(db_conn, "location_dim")
```

#### Load CSV

Function `OD_DL_csv` is created in order to access the necessary `CSV` files containing over 30,000 observations. This method of was select over loading from Github, do to storage limitations with files this large. shared file URL and csv names are stored into variables `url_location`, `url_job_openings`, `csv_location` and `csv_openings` respectively. 

```{r}
OD_DL_csv <- function(sharedURL, file_name, save2wd = FALSE){

   # Save the shared url 
   URL1 <- unlist(strsplit(sharedURL,"[?]"))[1]
   URL1 <- paste0(URL1,"?download=1") # edit URL to make it a downloadable link
   
   # Download the file to a temp directory using the supplied file name
   curl::curl_download(
      URL1,
      destfile = file.path(tempdir(), file_name),
      mode = "wb"
      )


   # If the user wants it saved to thier working directory this will copy the file
   if(isTRUE(save2wd)){
      file.copy(
         from = paste0(tempdir(),"\\", file_name),
         to = "./")
      }

   # return the CSV as a data.frame
   return(read.csv(paste0(tempdir(), "\\" ,file_name), stringsAsFactors = FALSE))

}

```

```{r, echo=FALSE}

url_location<-
   "https://cuny-my.sharepoint.com/:x:/g/personal/gabriel_campos77_qmail_cuny_edu/EYAX_5NCc_hFtSUFY0W4NPMBEXiRYtgro4V3vrSJbpTcag?e=aRKrpf"

url_job_openings<-
   "https://cuny-my.sharepoint.com/:x:/g/personal/gabriel_campos77_qmail_cuny_edu/EWni58F8o9BKqLwvF-FDdIEBX7nGv3l7PeueVkrUEPhJ_A?e=9hOyGN"

csv_locations<-
   "job_locations.csv"

csv_job_openings<-
   "job_openings.csv"
```

```{r}
head(url_location)
```

Calling the `OD_DL_csv()` function the csv files are loaded to `location_dim_df` and `job_opening_df`

```{r Source the location dimension file to load}
location_dim_df <- OD_DL_csv(sharedURL = url_location,
          file_name = csv_locations
          )

head(location_dim_df,5)
```


```{r Source the job Openings file to load}
job_opening_df <- OD_DL_csv(sharedURL = url_job_openings,
          file_name = csv_job_openings
          )

head(job_opening_df,5)
```

#### Write to Database

Using [dbWriteTable](https://dbi.r-dbi.org/reference/dbwritetable) function, we are able to write these imported dataframes into the appropriate Database tables on our local Database.

```{r Append new records to location dimension}
dbWriteTable(db_conn, name = "location_dim", value = location_dim_df, append = TRUE,row.names = FALSE)
```

```{r append new records to the job opening table}
dbWriteTable(db_conn, name = "job_opening_tbl", value = job_opening_df, append = TRUE, row.names = FALSE)
```

An [SQL in the Project 3 Repo](https://github.com/gcampos100/DATA607_CUNY_2021_Project3/tree/main/Gabe%20Folder) is used, to retreive the merged data, filtered by job titles, containg the word $DATA$.

```{r, echo=FALSE}
filename <-"https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Gabe%20Folder/jobs_location.sql"
```


```{r source sql file and substitute each new line "\n" with a space}
filename <-"https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Gabe%20Folder/jobs_location.sql"
db_sql <- read_file(filename)
db_sql <- gsub("\n", " ",db_sql)
db_sql
```

```{r execute sql query and return result set}
result_set = dbGetQuery(db_conn, db_sql)
#result_set = fetch(db_data, n = -1)
head(result_set,5)
```

```{r display the class of the result set}
class(result_set)
```

### Manipulating Imported Data

### Analysis
N-Gram function
<!-- This is Joe's N-Gram function used for analyses --->

```{r}
phrases <- result_set$job_descr
#view(phrases)

createNgram <-function(stringVector, ngramSize){

  ngram <- data.table()

  ng <- textcnt(stringVector, method = "string", n=ngramSize, tolower = FALSE)

  if(ngramSize==1){
    ngram <- data.table(w1 = names(ng), freq = unclass(ng), length=nchar(names(ng)))  
  }
  else {
    ngram <- data.table(w1w2 = names(ng), freq = unclass(ng), length=nchar(names(ng)))
  }
  return(ngram)
}
```

```{r}
text <- phrases
res <- createNgram(text,1)
names(res)
head(res %>% arrange(desc(freq)),20)

```


<!-- This is Joe's map of zip codes --->

Plotting zip codes

```{r, results='hide', echo=FALSE, warning=FALSE,message=FALSE}
top_states <- location_dim_df$zip_code

table(unlist(strsplit(tolower(top_states), " ")))
data.frame(table(unlist(strsplit(tolower(top_states), " "))))

```

```{r, rout.width = "100%", warning=FALSE}


top_states <- location_dim_df$zip_code

zip.plot(location_dim_df, zip.file = system.file("extdata", "zips.tab", package = 
"muRL"), map.type = "usa", cex = .2, col = "orange", pch = 2, 
jitter.factor = NULL)
```

<!-- This is where Joe ends --->



##### Conclusion



<!------- Below is for removing excessive space in Rmarkdown | HTML formatting -------->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>

