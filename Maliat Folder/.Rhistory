res[res$w1=="machine",]
res[res$w1=="master",]
res[res$w1=="bachelor",]
res[res$w1=="artificial",]
### Job searchers who have master's degree may have a better opportunity to get a job in the field of data analysis.
##wordcloud
library(wordcloud)
library(RColorBrewer)
library(wordcloud2)
set.seed(1234) # for reproducibility
wordcloud(words =res$w1 , freq = res$freq, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
# Chunk 1
library(devtools)
library(tidyverse)
library(RCurl)
library(plyr)
library(knitr)
library(RMySQL)
library(DSI)
library(odbc)
library(stringr)
library(tau)
library(data.table)
# Chunk 2: MySQL Connect Function
db_conn <- dbConnect(odbc(),"data607")
dbListTables(db_conn)
# Chunk 3: List the fields in the location dimension database table
# table location_dim fields are listed below
dbListFields(db_conn, "location_dim")
dbListFields(db_conn, "job_opening_tbl")
# Chunk 4
OD_DL_csv <- function(sharedURL, file_name, save2wd = FALSE){
# Save the shared url
URL1 <- unlist(strsplit(sharedURL,"[?]"))[1]
URL1 <- paste0(URL1,"?download=1") # edit URL to make it a downloadable link
# Download the file to a temp directory using the supplied file name
curl::curl_download(
URL1,
destfile = file.path(tempdir(), file_name),
mode = "wb"
)
# If the user wants it saved to thier working directory this will copy the file
if(isTRUE(save2wd)){
file.copy(
from = paste0(tempdir(),"\\", file_name),
to = "./")
}
# return the CSV as a data.frame
return(read.csv(paste0(tempdir(), "\\" ,file_name), stringsAsFactors = FALSE))
}
# Chunk 5
url_Cleaned_DS <- "https://cuny-my.sharepoint.com/:x:/g/personal/gabriel_campos77_qmail_cuny_edu/EdcuUpO6t1RIqyHw-OP90UQB2qRYY3fcW5RPyA_xT348Qw?e=4HeXbe"
url_DS_job_market<- "https://cuny-my.sharepoint.com/:x:/g/personal/gabriel_campos77_qmail_cuny_edu/Eai1ECgMhthLmrut-eIdD5oBw4715IvolGgyG54owY0nWg?e=D0iRKm"
url_location<-
"https://cuny-my.sharepoint.com/:x:/g/personal/gabriel_campos77_qmail_cuny_edu/EYAX_5NCc_hFtSUFY0W4NPMBEXiRYtgro4V3vrSJbpTcag?e=aRKrpf"
url_job_openings<-
"https://cuny-my.sharepoint.com/:x:/g/personal/gabriel_campos77_qmail_cuny_edu/EWni58F8o9BKqLwvF-FDdIEBX7nGv3l7PeueVkrUEPhJ_A?e=9hOyGN"
csv_locations<-
"job_locations.csv"
csv_job_openings<-
"job_openings.csv"
csv_DS_job_market <-
"alldata.csv"
csv_Cleaned_DS<- "Cleaned_DS_Jobs.csv"
# Chunk 6: Source the location dimension file to load
location_dim_df <- OD_DL_csv(sharedURL = url_location,
file_name = csv_locations
)
# Chunk 7: Source the job Openings file to load
job_opening_df <- OD_DL_csv(sharedURL = url_job_openings,
file_name = csv_job_openings
)
# Chunk 8: Source the job Openings file to load 2
# additional un tidy data set
DS_job_df <- OD_DL_csv(sharedURL = url_DS_job_market,
file_name = csv_DS_job_market
)
# Chunk 9
# to filter 30,000 rows in rmd and lighten the load to MySQL
job_opening_df <- job_opening_df %>%
filter(grepl('Data',job_title))
# Chunk 10
# to filter 30,000 rows in rmd and lighten the load to MySQL
location_dim_df <- location_dim_df %>%
filter(uniq_id %in% job_opening_df$uniq_id)
# Chunk 11
job_opening_df$job_descr <- gsub("<[^>]+>","",job_opening_df$job_descr)
# Chunk 12
# add unique ID column based on row name of each entry
DS_job_df<-cbind( data.frame("uniq_id" = as.integer(rownames(DS_job_df))), DS_job_df)
# add 40K to each uniq_id to ensure none match the first 30K entries of the other sets
DS_job_df<-DS_job_df %>%
mutate_at( vars("uniq_id") ,
funs(.+40000))
# Chunk 13
# splitting up the location column to create state, city, zip_code
temp<-as.data.frame(str_split_fixed(DS_job_df$location,",",2))
temp$V2<-str_trim(temp$V2)
temp$V2<-str_replace_all(temp$V2, "\\s", " == ")
DS_job_df<-(temp<-cbind(DS_job_df,"city" = temp$V1,
as.data.frame(str_split_fixed(temp$V2,"==",2)))%>%
dplyr::rename("state"=V1, "zip_code"=V2))
# loading country column with a constan "United State" value
DS_job_df$country<-"United States"
# quick rearrange
DS_job_df<-DS_job_df %>%
select(uniq_id,position,description,location,city,state,country,zip_code,company,reviews)
# Chunk 14
job_opening_df <-rbind(DS_job_df %>%
select(1,2,3) %>%
dplyr::rename( "uniq_id" = `uniq_id`,
"job_title" = `position`,
"job_descr" = `description`)
,job_opening_df)
# Chunk 15
location_dim_df <-rbind(DS_job_df %>%
select(1,4,5,6,7,8),location_dim_df)
# Chunk 16: Append new records to location dimension
#dbSendQuery(db_conn, "SET GLOBAL local_infile = true;")
dbWriteTable(db_conn, name = "location_dim", value = location_dim_df, append = TRUE,row.names = FALSE)
# Chunk 17: append new records to the job opening table
#dbSendQuery(db_conn, "SET GLOBAL local_infile = true;")
dbWriteTable(db_conn, name = "job_opening_tbl", value = job_opening_df, append = TRUE, row.names = FALSE)
# Chunk 18
filename <-"https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Gabe%20Folder/jobs_location.sql"
# Chunk 19: source sql file and substitute each new line "\n" with a space
filename <-"https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Gabe%20Folder/jobs_location.sql"
db_sql <- read_file(filename)
db_sql <- gsub("\n", " ",db_sql)
db_sql
# Chunk 20: execute sql query and return result set
result_set = dbGetQuery(db_conn, db_sql)
#result_set = fetch(db_data, n = -1)
head(result_set,5)
# Chunk 21: display the class of the result set
class(result_set)
# Chunk 22
#job_opening_df %>% mutate_at( vars( matches("job_desc")) ,~str_view( . ,"<.*.>") )
# Chunk 23
phrases <- result_set$job_descr
#view(phrases)
createNgram <-function(stringVector, ngramSize){
ngram <- data.table()
ng <- textcnt(stringVector, method = "string", n=ngramSize, tolower = FALSE)
if(ngramSize==1){
ngram <- data.table(w1 = names(ng), freq = unclass(ng), length=nchar(names(ng)))
}
else {
ngram <- data.table(w1w2 = names(ng), freq = unclass(ng), length=nchar(names(ng)))
}
return(ngram)
}
# Chunk 24
text <- phrases
res <- createNgram(text,1)
names(res)
head(res %>% arrange(desc(freq)),40)
# Chunk 25
res <- res %>%
arrange(desc(freq))
# Chunk 26
res[res$w1=="machine",]
res[res$w1=="master",]
res[res$w1=="bachelor",]
res[res$w1=="artificial",]
### Job searchers who have master's degree may have a better opportunity to get a job in the field of data analysis.
# Chunk 27
##wordcloud
library(wordcloud)
library(RColorBrewer)
library(wordcloud2)
# Chunk 28
set.seed(1234) # for reproducibility
wordcloud(words =res$w1 , freq = res$freq, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
wordcloud2(data=res, size = 0.7, shape = 'pentagon')
set.seed(1234) # for reproducibility
wordcloud(words =res$w1 , freq = res$freq, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
head(res)
##avoidedwords <- read_csv("https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Maliat%20Folder/avoidedwords.csv")
allWords = as.data.frame(table(res$w1))
wordsToAvoid = read.csv("https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Maliat%20Folder/avoidedwords.csv")
finalWords = anti_join(allWords, wordsToAvoid, by = "w1")
head(res)
res <- res %>%
arrange(desc(freq))
res[res$w1=="machine",]
res[res$w1=="master",]
res[res$w1=="bachelor",]
res[res$w1=="artificial",]
### Job searchers who have master's degree may have a better opportunity to get a job in the field of data analysis.
##avoidedwords <- read_csv("https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Maliat%20Folder/avoidedwords.csv")
allWords = as.data.frame(table(res$w1))
wordsToAvoid = read.csv("https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Maliat%20Folder/avoidedwords.csv")
finalWords = anti_join(allWords, wordsToAvoid, by = "w1")
##avoidedwords <- read_csv("https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Maliat%20Folder/avoidedwords.csv")
allWords = as.data.frame(table(res$w1))
wordsToAvoid = read.csv("https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Maliat%20Folder/avoidedwords.csv")
finalWords = anti_join(allWords, wordsToAvoid, by = "w1")
##avoidedwords <- read_csv("https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Maliat%20Folder/avoidedwords.csv")
allWords = as.data.frame(table(res$w1))
wordsToAvoid = read.csv("https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Maliat%20Folder/avoidedwords.csv")
finalWords = anti_join(allWords, wordsToAvoid)
##avoidedwords <- read_csv("https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Maliat%20Folder/avoidedwords.csv")
allWords = as.data.frame(table(res$w1))
wordsToAvoid = read.csv("https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Maliat%20Folder/avoidedwords.csv")
finalWords = anti_join(allWords, wordsToAvoid,by="w1")
# Chunk 1
library(devtools)
library(tidyverse)
library(RCurl)
library(plyr)
library(knitr)
library(RMySQL)
library(DSI)
library(odbc)
library(stringr)
library(tau)
library(data.table)
# Chunk 2: MySQL Connect Function
db_conn <- dbConnect(odbc(),"data607")
dbListTables(db_conn)
# Chunk 3: List the fields in the location dimension database table
# table location_dim fields are listed below
dbListFields(db_conn, "location_dim")
dbListFields(db_conn, "job_opening_tbl")
# Chunk 4
OD_DL_csv <- function(sharedURL, file_name, save2wd = FALSE){
# Save the shared url
URL1 <- unlist(strsplit(sharedURL,"[?]"))[1]
URL1 <- paste0(URL1,"?download=1") # edit URL to make it a downloadable link
# Download the file to a temp directory using the supplied file name
curl::curl_download(
URL1,
destfile = file.path(tempdir(), file_name),
mode = "wb"
)
# If the user wants it saved to thier working directory this will copy the file
if(isTRUE(save2wd)){
file.copy(
from = paste0(tempdir(),"\\", file_name),
to = "./")
}
# return the CSV as a data.frame
return(read.csv(paste0(tempdir(), "\\" ,file_name), stringsAsFactors = FALSE))
}
# Chunk 5
url_Cleaned_DS <- "https://cuny-my.sharepoint.com/:x:/g/personal/gabriel_campos77_qmail_cuny_edu/EdcuUpO6t1RIqyHw-OP90UQB2qRYY3fcW5RPyA_xT348Qw?e=4HeXbe"
url_DS_job_market<- "https://cuny-my.sharepoint.com/:x:/g/personal/gabriel_campos77_qmail_cuny_edu/Eai1ECgMhthLmrut-eIdD5oBw4715IvolGgyG54owY0nWg?e=D0iRKm"
url_location<-
"https://cuny-my.sharepoint.com/:x:/g/personal/gabriel_campos77_qmail_cuny_edu/EYAX_5NCc_hFtSUFY0W4NPMBEXiRYtgro4V3vrSJbpTcag?e=aRKrpf"
url_job_openings<-
"https://cuny-my.sharepoint.com/:x:/g/personal/gabriel_campos77_qmail_cuny_edu/EWni58F8o9BKqLwvF-FDdIEBX7nGv3l7PeueVkrUEPhJ_A?e=9hOyGN"
csv_locations<-
"job_locations.csv"
csv_job_openings<-
"job_openings.csv"
csv_DS_job_market <-
"alldata.csv"
csv_Cleaned_DS<- "Cleaned_DS_Jobs.csv"
# Chunk 6: Source the location dimension file to load
location_dim_df <- OD_DL_csv(sharedURL = url_location,
file_name = csv_locations
)
# Chunk 7: Source the job Openings file to load
job_opening_df <- OD_DL_csv(sharedURL = url_job_openings,
file_name = csv_job_openings
)
# Chunk 8: Source the job Openings file to load 2
# additional un tidy data set
DS_job_df <- OD_DL_csv(sharedURL = url_DS_job_market,
file_name = csv_DS_job_market
)
# Chunk 9
# to filter 30,000 rows in rmd and lighten the load to MySQL
job_opening_df <- job_opening_df %>%
filter(grepl('Data',job_title))
# Chunk 10
# to filter 30,000 rows in rmd and lighten the load to MySQL
location_dim_df <- location_dim_df %>%
filter(uniq_id %in% job_opening_df$uniq_id)
# Chunk 11
job_opening_df$job_descr <- gsub("<[^>]+>","",job_opening_df$job_descr)
# Chunk 12
# add unique ID column based on row name of each entry
DS_job_df<-cbind( data.frame("uniq_id" = as.integer(rownames(DS_job_df))), DS_job_df)
# add 40K to each uniq_id to ensure none match the first 30K entries of the other sets
DS_job_df<-DS_job_df %>%
mutate_at( vars("uniq_id") ,
funs(.+40000))
# Chunk 13
# splitting up the location column to create state, city, zip_code
temp<-as.data.frame(str_split_fixed(DS_job_df$location,",",2))
temp$V2<-str_trim(temp$V2)
temp$V2<-str_replace_all(temp$V2, "\\s", " == ")
DS_job_df<-(temp<-cbind(DS_job_df,"city" = temp$V1,
as.data.frame(str_split_fixed(temp$V2,"==",2)))%>%
dplyr::rename("state"=V1, "zip_code"=V2))
# loading country column with a constan "United State" value
DS_job_df$country<-"United States"
# quick rearrange
DS_job_df<-DS_job_df %>%
select(uniq_id,position,description,location,city,state,country,zip_code,company,reviews)
# Chunk 14
job_opening_df <-rbind(DS_job_df %>%
select(1,2,3) %>%
dplyr::rename( "uniq_id" = `uniq_id`,
"job_title" = `position`,
"job_descr" = `description`)
,job_opening_df)
# Chunk 15
location_dim_df <-rbind(DS_job_df %>%
select(1,4,5,6,7,8),location_dim_df)
# Chunk 16: Append new records to location dimension
#dbSendQuery(db_conn, "SET GLOBAL local_infile = true;")
dbWriteTable(db_conn, name = "location_dim", value = location_dim_df, append = TRUE,row.names = FALSE)
# Chunk 17: append new records to the job opening table
#dbSendQuery(db_conn, "SET GLOBAL local_infile = true;")
dbWriteTable(db_conn, name = "job_opening_tbl", value = job_opening_df, append = TRUE, row.names = FALSE)
# Chunk 18
filename <-"https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Gabe%20Folder/jobs_location.sql"
# Chunk 19: source sql file and substitute each new line "\n" with a space
filename <-"https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Gabe%20Folder/jobs_location.sql"
db_sql <- read_file(filename)
db_sql <- gsub("\n", " ",db_sql)
db_sql
# Chunk 20: execute sql query and return result set
result_set = dbGetQuery(db_conn, db_sql)
#result_set = fetch(db_data, n = -1)
head(result_set,5)
# Chunk 21: display the class of the result set
class(result_set)
# Chunk 22
#job_opening_df %>% mutate_at( vars( matches("job_desc")) ,~str_view( . ,"<.*.>") )
# Chunk 23
phrases <- result_set$job_descr
#view(phrases)
createNgram <-function(stringVector, ngramSize){
ngram <- data.table()
ng <- textcnt(stringVector, method = "string", n=ngramSize, tolower = FALSE)
if(ngramSize==1){
ngram <- data.table(w1 = names(ng), freq = unclass(ng), length=nchar(names(ng)))
}
else {
ngram <- data.table(w1w2 = names(ng), freq = unclass(ng), length=nchar(names(ng)))
}
return(ngram)
}
# Chunk 24
text <- phrases
res <- createNgram(text,1)
names(res)
head(res %>% arrange(desc(freq)),40)
# Chunk 25
res <- res %>%
arrange(desc(freq))
# Chunk 26
res[res$w1=="machine",]
res[res$w1=="master",]
res[res$w1=="bachelor",]
res[res$w1=="artificial",]
### Job searchers who have master's degree may have a better opportunity to get a job in the field of data analysis.
# Chunk 27
##wordcloud
library(wordcloud)
library(RColorBrewer)
library(wordcloud2)
# Chunk 28
set.seed(1234) # for reproducibility
wordcloud(words =res$w1 , freq = res$freq, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
# Chunk 29
wordcloud2(data=res, size = 0.7, shape = 'pentagon')
# Chunk 30
head(res)
# Chunk 31
##avoidedwords <- read_csv("https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Maliat%20Folder/avoidedwords.csv")
allWords = as.data.frame(table(res$w1))
wordsToAvoid = read.csv("https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Maliat%20Folder/avoidedwords.csv")
finalWords = anti_join(allWords, wordsToAvoid,by="w1")
###avoiding with,the,and,of,in:
allWords = as.data.frame(table(res$w1))
wordsToAvoid = read.csv("https://raw.githubusercontent.com/gcampos100/DATA607_CUNY_2021_Project3/main/Maliat%20Folder/avoidedwords.csv")
finalWords = anti_join(allWords, wordsToAvoid,by="w1")
